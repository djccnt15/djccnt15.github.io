---
published: true
layout: post
title: '[ê¸°ì´ˆí†µê³„í•™] 10. ë‹¤ì–‘í•œ ì´ì‚° í™•ë¥  ë¶„í¬'
description: >
    ë² ë¥´ëˆ„ì´ ë¶„í¬, ì´í•­ ë¶„í¬, ì´ˆê¸°í•˜ ë¶„í¬, í¬ì•„ì†¡ ë¶„í¬, ê¸°í•˜ ë¶„í¬, ìŒì´í•­ ë¶„í¬, ë‹¤í•­ ë¶„í¬
categories: [Statistics]
tags: [statistics]
image:
    path: /assets/img/posts/thumbnail_statistics_10.png
related_posts:
    - _posts/statistics/2023-01-01-statistics_09.md
    - _posts/statistics/2023-02-11-statistics_11.md
---
{% include series_statistics.html %}
* toc
{:toc}

## 1. ë² ë¥´ëˆ„ì´ ë¶„í¬

ë‹¤ìŒì˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì‹¤í—˜ì„ **ë² ë¥´ëˆ„ì´ ì‹œí–‰(Bernoulli trial)**ì´ë¼ í•œë‹¤.  

- ê° ì‹¤í—˜ì—ì„œ ë°œìƒ ê°€ëŠ¥í•œ ê²°ê³¼ê°€ ë‹¨ ë‘ ê°€ì§€
- ê° ì‹¤í—˜ì€ ë…ë¦½ì ìœ¼ë¡œ ìˆ˜í–‰
- ëª¨ë“  ì‹¤í—˜ì—ì„œ ê²°ê³¼ì˜ í™•ë¥ ì€ í•­ìƒ ë™ì¼

ğŸ’¡ëª¨ì§‘ë‹¨ì´ ì¶©ë¶„íˆ í¬ê³  í‘œë³¸í¬ê¸°ê°€ ìƒëŒ€ì ìœ¼ë¡œ í¬ì§€ ì•Šì€ ê²½ìš° ë¹„ë³µì›ì¶”ì¶œë„ ë² ë¥´ëˆ„ì´ ì‹¤í—˜ì„ ê·¼ì‚¬ëª¨í˜•ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤.  
{:.note}

ëª¨ìˆ˜(parameter)ì¸ ì„±ê³µ í™•ë¥ ì´ $$p$$ì¸ ë² ë¥´ëˆ„ì´ ì‹œí–‰ì˜ í™•ë¥  ë³€ìˆ˜ì˜ ë¶„í¬ë¥¼ **ë² ë¥´ëˆ„ì´ ë¶„í¬(Bernoulli distribution)**ë¼ í•˜ê³ , ì•„ë˜ì™€ ê°™ì´ í‘œê¸°í•œë‹¤.  

$$X \sim B(p)$$

ë² ë¥´ëˆ„ì´ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ì˜ ì¼ë°˜ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.  

$$f(x) = P(X = x) = p^{x}(1 - p)^{1 - x}, \quad x = 0, 1$$

ë² ë¥´ëˆ„ì´ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ Pythonìœ¼ë¡œ êµ¬í˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.  

```python
def bernoulli_d(p: float, x: int = 0 | 1, n: int = 1) -> float:
    """
    returns probability of bernoulli distribution
    x: case
    p: probability
    """

    return (p ** x) * ((1 - p) ** (n - x))
```

SciPyë¥¼ ì‚¬ìš©í•˜ë©´ ë² ë¥´ëˆ„ì´ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ ê²°ê³¼ë¥¼ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤.  

```python
from scipy.stats import bernoulli

print(bernoulli.pmf(k=1, p=0.2))
```

ë² ë¥´ëˆ„ì´ ë¶„í¬ì˜ ê¸°ëŒ€ê°’ê³¼ ë¶„ì‚°ì€ ì•„ë˜ì™€ ê°™ë‹¤.  

$$\begin{align*}
& E(X) = 0 \times (1 - p) + 1 \times p = p \\
\\
& E(X^{2}) = 0^{2} \times (1 - p) + 1^{2} \times p = p \\
\\
& Var(X) = p - p^{2} = p(1 - p) \\
\\
& SD(X) = \sqrt{p(1 - p)}
\end{align*}$$

## 2. ì´í•­ ë¶„í¬

ì„±ê³µ í™•ë¥ ì´ $$p$$ì¸ ë² ë¥´ëˆ„ì´ ì‹¤í—˜ì„ $$n$$ë²ˆ ë°˜ë³µí–ˆì„ ë•Œ, ì„±ê³µ íšŸìˆ˜ $$X$$ì˜ ë¶„í¬ë¥¼ **ì´í•­ ë¶„í¬(binomial distribution)**ë¼ í•œë‹¤.  

$$X_{i} \sim B(p)$$ë¼ê³  í•  ë•Œ, ì„±ê³µ íšŸìˆ˜ $$X$$ëŠ” $$n$$ê°œì˜ ë² ë¥´ëˆ„ì´ í™•ë¥  ë³€ìˆ˜ì˜ í•©ìœ¼ë¡œ í‘œì‹œí•œë‹¤.  

$$X = X_{1} + X_{2} + \cdots + X_{n}$$

ë”°ë¼ì„œ [ë…ë¦½ì¸ ê²°í•© ë¶„í¬ì˜ ì„±ì§ˆ](/statistics/statistics_09/#2-ê³µë¶„ì‚°ê³¼-ìƒê´€ê³„ìˆ˜)ì„ ë°”íƒ•ìœ¼ë¡œ ì´í•­ ë¶„í¬ì˜ ê¸°ëŒ€ê°’ê³¼ ë¶„ì‚°ì„ ìœ ë„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.  

$$\begin{align*}
E(X_{i}) = p \ & \to \ E(X) = np \\
\\
Var(X_{i}) = p(1 - p) \ & \to \ Var(X) = np(1 - p) \\
\\
SD(X_{i}) = \sqrt{p(1 - p)} \ & \to \ SD(X) = \sqrt{np(1 - p)}
\end{align*}$$

ì‹œí–‰ íšŸìˆ˜ë¥¼ $$n$$, ì„±ê³µ í™•ë¥  $$p$$ì¸ ì´í•­ ë¶„í¬ë¥¼ ì•„ë˜ì™€ ê°™ì´ í‘œê¸°í•œë‹¤.  

$$X \sim B(n, p)$$

ì´í•­ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ì˜ ì¼ë°˜ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.  

$$f(x) = \binom{n}{x}p^{x}(1 - p)^{n - x}, \quad x = 0, 1, \cdots, n$$

ì´í•­ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ Pythonìœ¼ë¡œ êµ¬í˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.  

```python
def binom_d(x: int, n: int, p: float) -> float:
    """
    returns probability of binom distribution
    x: case
    n: number of trial
    p: probability
    """

    return combination(n, x) * bernoulli_d(x=x, n=n, p=p)


def binom_c(x: int, n: int, p: float, start: int = 0) -> float:
    """
    returns cumulative probability of binom distribution
    x: case
    n: number of trial
    p: probability
    """

    return sum(binom_d(i, n, p) for i in range(start, x + 1))
```

SciPyë¥¼ ì‚¬ìš©í•˜ë©´ ì´í•­ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ ê²°ê³¼ë¥¼ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤.  

```python
from scipy.stats import binom

print(binom.pmf(k=8, n=15, p=0.5))
print(binom.cdf(k=8, n=15, p=0.5))
```

$$X \sim B(m, p), Y \sim B(n, p)$$ì´ê³  $$X, Y$$ê°€ ë…ë¦½ì¸ ê²½ìš° ì´í•­ ë¶„í¬ì˜ ê²°í•©ì€ ì•„ë˜ì™€ ê°™ë‹¤.  

$$X + Y \sim B(m + n, p)$$

NumPyë¥¼ ì‚¬ìš©í•˜ë©´ ì´í•­ ë¶„í¬í•˜ëŠ” í‘œë³¸ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆë‹¤.  

```python
import numpy as np

data = np.random.default_rng(seed=0).binomial(n=10, p=0.5, size=1000)
```

## 3. ì´ˆê¸°í•˜ ë¶„í¬

ê° ì‹¤í—˜ì—ì„œ ë°œìƒ ê°€ëŠ¥í•œ ê²°ê³¼ê°€ ë‹¨ ë‘ ê°€ì§€ì´ê³ , í¬ê¸°ê°€ $$N$$ì¸ ëª¨ì§‘ë‹¨(ìœ í•œëª¨ì§‘ë‹¨)ì´ ê°ê° $$M$$ê³¼ $$N - M$$ í¬ê¸°ì˜ ë¶€ëª¨ì§‘ë‹¨ $$A, B$$ë¡œ ë‚˜ë‰˜ì–´ì§„ ê²½ìš°ì—ì„œ $$n$$ê°œì˜ í‘œë³¸ì„ ë¬´ì‘ìœ„ë¡œ ë¹„ë³µì›ì¶”ì¶œí•  ë•Œ, ë¶€ëª¨ì§‘ë‹¨ $$A$$ì—ì„œ ì¶”ì¶œëœ í‘œë³¸ ìˆ˜ì˜ ë¶„í¬ë¥¼ **ì´ˆê¸°í•˜ ë¶„í¬(hypergeometric distribution)**ë¼ í•œë‹¤.  

ì´ˆê¸°í•˜ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ì˜ ì¼ë°˜ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.  

$$f(x) = \frac{\binom{M}{x}\binom{N - M}{n - x}}{\binom{N}{n}}, \quad x = max(0, n - N + M), \cdots, min(n, M)$$

ì´ˆê¸°í•˜ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ Pythonìœ¼ë¡œ êµ¬í˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.  

```python
def hyper_d(x: int, M: int, n: int, N: int) -> float:
    """
    returns probability of hypergeometric distribution
    x: case
    M: size of subpopulation
    n: size of sample
    N: size of population
    """

    return combination(M, x) * combination(N - M, n - x) / combination(N, n)


def hyper_c(x: int, M: int, n: int, N: int, start: int = 0) -> float:
    """
    returns cumulative probability of hypergeometric distribution
    x: case
    M: size of subpopulation
    n: size of sample
    N: size of population
    """

    return sum(hyper_d(x=x, n=n, N=N, M=M) for x in range(start, x + 1))
```

SciPyë¥¼ ì‚¬ìš©í•˜ë©´ ì´ˆê¸°í•˜ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ ê²°ê³¼ê°’ì„ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤.  

```python
from scipy.stats import hypergeom

k = 1  # case
n = 4  # size of subpopulation
N = 3  # size of sample
M = 10  # size of population

print(hypergeom.pmf(k=k, n=n, N=N, M=M))
print(hypergeom.cdf(k=k, n=n, N=N, M=M))
```

â—SciPyëŠ” ìˆ˜ì‹ í‘œê¸°ê°€ ì¡°ê¸ˆ ë‹¬ë¼ ì£¼ì˜í•´ì•¼ í•œë‹¤. SciPyì—ì„œ ì‚¬ìš©í•˜ëŠ” ì´ˆê¸°í•˜ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.  
{:.note title='attention'}

$$p(k, M, n, N) = \frac{\binom{n}{k}\binom{M - n}{N - k}}{\binom{M}{N}}$$

$$N$$ì´ í¬ê³  $$N$$ì— ë¹„í•´ $$n$$ì´ ìƒëŒ€ì ìœ¼ë¡œ ë§¤ìš° ì‘ì€ ê²½ìš°($$n \ll N$$) ë¹„ë³µì›ì˜ íš¨ê³¼ê°€ ì ê¸° ë•Œë¬¸ì— ë² ë¥´ëˆ„ì´ ì‹¤í—˜ìœ¼ë¡œ ê·¼ì‚¬í•˜ë©°, ë”°ë¼ì„œ ì´ˆê¸°í•˜ ë¶„í¬ ì—­ì‹œ $$p = M/N$$ì¸ ì´í•­ ë¶„í¬ë¡œ ê·¼ì‚¬í•œë‹¤.  

ì´ˆê¸°í•˜ ë¶„í¬ì˜ ê¸°ëŒ€ê°’ê³¼ ë¶„ì‚°ì€ ì•„ë˜ì™€ ê°™ë‹¤.  

$$\begin{align*}
E(X) & = n \frac{M}{N} = np \\
\\
Var(X) & = np(1 - p) - n(n - 1)\frac{p(1 - p)}{N - 1} \\
\\
& = np(1 - p)\frac{N - n}{N - 1} = n\frac{M}{N} \left( 1 - \frac{M}{N} \right) \leq np(1 - p)
\end{align*}$$

<details><summary>ì´ˆê¸°í•˜ ë¶„í¬ ê¸°ëŒ€ê°’ê³¼ ë¶„ì‚°ì˜ ìœ ë„</summary><div markdown="1">

$$E(X_{i}) = \frac{M}{N} = p \ \to \ E(X) = n \frac{M}{N} = np$$

$$\begin{align*}
E(X_{i}) & = \frac{M}{N} = p, \quad E(X_{i}^{2}) = \frac{M}{N} = p \\
\\
\therefore Var(X_{i}) & = p - p^{2} = p(1 - p) = \frac{M}{N}\frac{N - M}{N} \\
\\
\therefore Var(X) & = \sum_{i}Var(X_{i}) + 2\sum_{i < j}Cov(X_{i}, X_{j}) \\
\\
Cov(X_{i}, X_{j}) & = E(X_{i}X_{j}) - E(X_{i})E(X_{j}) \\
\\
E(X_{i}X_{j}) & = P(X_{i} = 1, X_{j} = 1) \quad \because X_{i} = 0 \ \to \ E(X_{i}X_{j}) = 0 \\
& = P(X_{i} = 1)P({X_{j} = 1 \vert X_{i} = 1}) = \frac{M}{N}\frac{M - 1}{N - 1} \\
\\
\therefore Cov(X_{i}, X_{j}) & = \frac{M}{N}\frac{M - 1}{N - 1} - \left( \frac{M}{N} \right)^{2} \\
& = -\frac{M}{N}\frac{N - M}{N(N - 1)} = -\frac{p(1 - p)}{N - 1} \leq 0 \\
\\
\therefore Var(X) & = np(1 - p) - n(n - 1)\frac{p(1 - p)}{N - 1} \\
& = np(1 - p)\frac{N - n}{N - 1}
\end{align*}$$

</div></details><br>

ì´ ë•Œ ìœ„ ì‹ì—ì„œ $$\frac{N - n}{N - 1}$$ì„ ìœ í•œëª¨ì§‘ë‹¨ ìˆ˜ì •ê³„ìˆ˜ë¼ í•˜ëŠ”ë°, ìœ í•œëª¨ì§‘ë‹¨ìœ¼ë¡œë¶€í„° ë¹„ë³µì›ì¶”ì¶œì„ í•˜ë©´ì„œ ë¶„ì‚°ì´ ì‘ì•„ì§„ë‹¤ëŠ” ê²ƒì€ í¼ì ¸ìˆëŠ” ì •ë„ê°€ ì‘ì•„ì ¸ ë°ì´í„°ì˜ ë³€ë™ì„±ì´ ì ì–´ì§„ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ìˆ˜ë¥¼ ì¶”ì •í–ˆì„ ë•Œ ë” ì•ˆì •ì ì¸ í˜•íƒœë¥¼ ê°–ëŠ”ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.  

NumPyë¥¼ ì‚¬ìš©í•˜ë©´ ì´ˆê¸°í•˜ ë¶„í¬í•˜ëŠ” í‘œë³¸ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆë‹¤.  

```python
import numpy as np

data = np.random.default_rng(seed=0).hypergeometric(ngood=20, nbad=20, nsample=10, size=100)
```

## 4. í¬ì•„ì†¡ ë¶„í¬

[ì´í•­ ë¶„í¬](#2-ì´í•­-ë¶„í¬)ì—ì„œ $$n$$ì´ ë§¤ìš° ì»¤ì§€ë©´ ê³„ì‚°ì— ì–´ë ¤ì›€ì´ ìƒê¸°ëŠ”ë°, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í¬ì•„ì†¡ ë¶„í¬ë¥¼ ì‚¬ìš©í•œë‹¤.  

í™•ë¥  ë³€ìˆ˜ $$X$$ê°€ ì´í•­ ë¶„í¬ $$B(n, p)$$ë¥¼ ë”°ë¥¼ ë•Œ $$p$$ê°€ ë§¤ìš° ì‘ìœ¼ë©´ í° $$x$$ì— ëŒ€í•œ í™•ë¥ ì€ ë¬´ì‹œí•  ì •ë„ë¡œ ì‘ì•„ì§€ëŠ”ë°, ì´ ê²½ìš°ì˜ í™•ë¥  ë¶„í¬ë¥¼ **í¬ì•„ì†¡ ë¶„í¬(Poisson distribution)**ë¼ í•˜ë©° ì•„ë˜ì™€ ê°™ì´ í‘œê¸°í•œë‹¤.  

$$X \sim \text{Pois}(\lambda)$$

ì¦‰, ë°œìƒ ê°€ëŠ¥ì„±ì´ í¬ë°•í•œ ì‚¬ê±´ì´ ì„ì˜ì˜ êµ¬ê°„ì—ì„œ í‰ê· ì ìœ¼ë¡œ $$\lambda$$ë²ˆ ë°œìƒí•˜ëŠ” ìƒí™©ì—ì„œ, êµ¬ê°„ì„ ë‚˜ëˆ„ì—ˆì„ ë•Œ ê° êµ¬ê°„ì˜ ë°œìƒ ë¹ˆë„ê°€ ì„œë¡œ ë…ë¦½(independent increment)ì´ê³  êµ¬ê°„ì˜ ìœ„ì¹˜ì™€ ê´€ê³„ì—†ì´ ë™ì¼ ê¸¸ì´ì˜ êµ¬ê°„ì—ì„œì˜ í‰ê· ë°œìƒ ë¹ˆë„ê°€ ë™ì¼(stationary increment)í•˜ë©´ í•´ë‹¹ ë¶„í¬ëŠ” í¬ì•„ì†¡ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.  

í¬ì•„ì†¡ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.  

$$f(x) = \binom{n}{x}p^{x}(1 - p)^{n - x} \simeq \frac{e^{-\lambda}\lambda^{x}}{x!}, \quad \lambda = np = E(X)$$

<details><summary>í¬ì•„ì†¡ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ ìœ ë„</summary><div markdown="1"><br>

ì´í•­ ë¶„í¬í•˜ëŠ” í™•ë¥  ë³€ìˆ˜ $$X$$ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ $$\lambda$$ë¥¼ ì´ìš©í•´ì„œ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.  

$$\begin{gathered}
E(X) = \lambda = np \ \to \ p = \frac{\lambda}{n} \\
\\
\Rightarrow f(x) = \binom{n}{x}p^{x}(1 - p)^{n - x} = \frac{n!}{x!(n - x)!} \left( \frac{\lambda}{n} \right)^{x} \left( 1 - \frac{\lambda}{n} \right)^{n - x}
\end{gathered}$$

ìœ„ ì‹ì—ì„œ $$n$$ì´ ë¬´í•œëŒ€ë¡œ ë°œì‚°í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ì •ë¦¬ëœë‹¤.  

$$\begin{gathered}
\frac{n!}{(n - x)!n^{x}} = \frac{n(n - 1) \cdots (n - x + 1)}{n^{x}} \ \to \ 1 \\
\\
\lim_{n \to \infty} \left( 1 - \frac{\lambda}{n} \right)^{n} = e^{-\lambda}, \quad \because \lim_{n \to \infty} \left( 1 + \frac{x}{n} \right)^{n} = e^{x} \\
\\
\lim_{n \to \infty} \left( 1 - \frac{\lambda}{n} \right)^{-x} = 1 \\
\\
\Rightarrow f(x) = \binom{n}{x}p^{x}(1 - p)^{n - x} \simeq \frac{e^{-\lambda}\lambda^{x}}{x!}
\end{gathered}$$

</div></details><br>

ğŸ’¡$$p$$ê°€ ì»¤ì§ˆìˆ˜ë¡ í¬ì•„ì†¡ ê·¼ì‚¬ì™€ ì´í•­ ë¶„í¬ì˜ ì˜¤ì°¨ê°€ ì»¤ì§€ëŠ”ë°, ì¼ë°˜ì ìœ¼ë¡œ $$\lambda$$ ê°’ì´ 5ë³´ë‹¤ ì‘ìœ¼ë©´ í¬ì•„ì†¡ ê·¼ì‚¬ë¥¼ ì‚¬ìš©í•´ë„ í° ë¬¸ì œê°€ ì—†ë‹¤ê³  í•œë‹¤.  
{:.note}

í¬ì•„ì†¡ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ Pythonìœ¼ë¡œ êµ¬í˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.  

```python
import math


def pois_d(x: int, l: float) -> float:
    """
    returns probability of poisson distribution
    x: case
    l: lambda, expectation of random variable
    """

    return (math.e ** -l) * (l ** x) / factorial(x)


def pois_c(x: int, l: float, start: int = 0) -> float:
    """
    returns cumulative probability of poisson distribution
    x: case
    l: lambda, expectation of random variable
    """

    return sum(pois_d(i, l) for i in range(start, x + 1))
```

SciPyë¥¼ ì‚¬ìš©í•˜ë©´ í¬ì•„ì†¡ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ ê²°ê³¼ê°’ì„ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤.  

```python
from scipy.stats import poisson

print(poisson.pmf(k=2, mu=2))
print(poisson.cdf(k=2, mu=2))
```

$$X \sim \text{Pois}(\lambda_{1}), Y \sim \text{Pois}(\lambda_{2})$$ì´ê³ , $$X, Y$$ê°€ ë…ë¦½ì¸ ê²½ìš° í¬ì•„ì†¡ ë¶„í¬ì˜ ê²°í•©ì€ ì•„ë˜ì™€ ê°™ë‹¤.  

$$X + Y \sim \text{Pois}(\lambda_{1} + \lambda_{2})$$

í¬ì•„ì†¡ ë¶„í¬ì˜ ê¸°ëŒ€ê°’ê³¼ ë¶„ì‚°ì€ ì•„ë˜ì™€ ê°™ë‹¤.  

$$\begin{align*}
E(X) & = \lambda \\
\\
Var(X) & = \lambda
\end{align*}$$

<details><summary>í¬ì•„ì†¡ ë¶„í¬ ê¸°ëŒ€ê°’ê³¼ ë¶„ì‚°ì˜ ìœ ë„</summary><div markdown="1">

$$\begin{align*}
E(X) & = \sum_{x=0}^{\infty}x\frac{e^{-\lambda}\lambda^{x}}{x!} = \sum_{x=1}^{\infty}x\frac{e^{-\lambda}\lambda^{x}}{x!} \\
\\
& = \lambda\sum_{x=1}^{\infty}\frac{e^{-\lambda}\lambda^{x - 1}}{(x - 1)!} \\
\\
y = x - 1 & \ \to \ \lambda\sum_{x=1}^{\infty}\frac{e^{-\lambda}\lambda^{x - 1}}{(x - 1)!} = \lambda\sum_{y=0}^{\infty}\frac{e^{-\lambda}\lambda^{y}}{y!} = \lambda
\end{align*}$$

$$\begin{align*}
Var(X) & = E(X^{2}) - E(X)^{2} \\
\\
& = E(X(X - 1)) + E(X) - E(X)^{2} \\
\\
& = \lambda^{2} + \lambda - \lambda^{2} = \lambda \\
\\
\because E(X(X - 1)) & = E(X^{2}) - E(X) \\
\\
& = \sum_{x=0}^{\infty}x(x - 1)\frac{e^{-\lambda}\lambda^{x}}{x!} = \lambda^{2}\sum_{x=2}^{\infty}\frac{e^{-\lambda}\lambda^{x - 2}}{(x - 2)!} \\
\\
y = x - 2 & \ \to \ \lambda^{2}\sum_{y=0}^{\infty}\frac{e^{-\lambda}\lambda^{y}}{y!} = \lambda^{2}
\end{align*}$$

</div></details><br>

ğŸ’¡ë”°ë¼ì„œ ëª¨ì§‘ë‹¨ì´ í¬ì•„ì†¡ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ë©´, ê¸°ëŒ€ê°’ê³¼ ë¶„ì‚°ì´ ë¹„ìŠ·í•œ ê°’ì„ ê°€ì§„ë‹¤ê³  ì˜ˆìƒí•  ìˆ˜ ìˆìœ¼ë©°,  
â—í‰ê· ì— ë¹„í•´ í‘œë³¸ë¶„ì‚°ì´ ë§¤ìš° í¬ë‹¤ë©´($$\overline{x} \ll s^{2}$$) ë°ì´í„°ê°€ í¬ì•„ì†¡ ë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•Šì„ ê²ƒì´ë¼ê³  ì˜ˆìƒí•  ìˆ˜ ìˆë‹¤.  
{:.note}

NumPyë¥¼ ì‚¬ìš©í•˜ë©´ í¬ì•„ì†¡ ë¶„í¬í•˜ëŠ” í‘œë³¸ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆë‹¤.  

```python
import numpy as np

data = np.random.default_rng().poisson(lam=5, size=1000)
```

## 5. ê¸°í•˜ ë¶„í¬

ì„±ê³µ í™•ë¥ ì´ $$p$$ì¸ [ë² ë¥´ëˆ„ì´ ì‹œí–‰](#1-ë² ë¥´ëˆ„ì´-ë¶„í¬)ì„ ì„±ê³µí•  ë•Œê¹Œì§€ ì‹œí–‰í•˜ëŠ” ê²½ìš° ì‹¤íŒ¨(ì‹œí–‰) íšŸìˆ˜ì˜ ë¶„í¬ë¥¼ **ê¸°í•˜ ë¶„í¬(geometric distribution)**ë¼ í•˜ê³ , ì•„ë˜ì™€ ê°™ì´ í‘œê¸°í•œë‹¤.  

$$X \sim \text{Geo}(p)$$

ê¸°í•˜ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.  

$$f(x) = (1 - p)^{x}p, \quad x = 0, 1, 2, \cdots$$

ìœ„ì™€ ê°™ì´ 1í•­ì´ $$p$$ì´ê³  ê³µë¹„ê°€ $$1 - p$$ì¸ ë“±ë¹„ê¸‰ìˆ˜ í˜•íƒœë¥¼ ê°–ê³  ìˆê¸° ë•Œë¬¸ì— $$x$$ ë²ˆì§¸ ì‹¤í—˜ ì´ì „ì— ì„±ê³µí•  í™•ë¥ ì„ ì˜ë¯¸í•˜ëŠ” ê¸°í•˜ ë¶„í¬ì˜ ëˆ„ì ë¶„í¬í•¨ìˆ˜ëŠ” ë“±ë¹„ê¸‰ìˆ˜ì˜ í•©ì„ ê¸°ë°˜ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì´ ìœ ë„í•  ìˆ˜ ìˆë‹¤.  

$$\begin{align*}
P(X \leq x) & = \sum_{k=0}^{x}p(1 - p)^{k} = \frac{p - p(1 - p)^{x + 1}}{1 - (1 - p)} = 1 - (1 - p)^{x + 1} \\
\\
P(X \geq x) & = 1 - P(X \leq x - 1) = (1 - p)^{x}
\end{align*}$$

ê¸°í•˜ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ Pythonìœ¼ë¡œ êµ¬í˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.  

```python
def geom_d(x: int, p: float) -> float:
    """
    returns probability of geometric distribution
    x: number of failures
    p: probability
    """

    return ((1 - p) ** x) * p


def geom_c(x: int, p: float) -> float:
    """
    returns cumulative probability of geometric distribution
    x: number of failures
    p: probability
    """

    return 1 - ((1 - p) ** (x + 1))
```

ê¸°í•˜ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ ì‹œí–‰ íšŸìˆ˜ì— ëŒ€í•œ ì‹ìœ¼ë¡œ ë³€í™˜í•˜ë©´, ì‹œí–‰ íšŸìˆ˜ $$Y$$ëŠ” ì‹¤íŒ¨ íšŸìˆ˜ $$X + 1$$ê³¼ ë™ì¼í•˜ê¸° ë•Œë¬¸ì— ì•„ë˜ì™€ ê°™ì´ ë³€í˜•í•  ìˆ˜ ìˆë‹¤.  

$$Y = X + 1 \ \to \ f_{Y}(y) = (1 - p)^{y - 1}p, \quad y = 1, 2, \cdots$$

SciPyë¥¼ ì‚¬ìš©í•˜ë©´ ì‹œí–‰ íšŸìˆ˜ $$Y$$ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ $$f_{Y}(y)$$ì˜ ê²°ê³¼ê°’ì„ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤.  

```python
from scipy.stats import geom

print(geom.pmf(k=4, p=0.3))
print(geom.cdf(k=6, p=0.3))
```

$$x$$ ë²ˆì§¸ ì‹¤í—˜ ì´ì „ì— ì„±ê³µí•  í™•ë¥ ì€ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.  

$$P(Y \leq x) = P(X \leq x - 1)$$

ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹œí–‰ íšŸìˆ˜ì— ëŒ€í•œ ëˆ„ì ë¶„í¬í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.  

$$\begin{align*}
P(Y \leq y) & = P(X + 1 \leq y) = P(X \leq y - 1) = 1 - (1 - p)^{y} \\
\\
P(Y > y) & = 1 - P(Y \leq y) = (1 - p)^{y}
\end{align*}$$

ê¸°í•˜ ë¶„í¬ì˜ ê¸°ëŒ€ê°’ì€ ë¬´í•œë“±ë¹„ê¸‰ìˆ˜ì˜ í•©ì„ ê¸°ë°˜ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì´ ìœ ë„í•  ìˆ˜ ìˆë‹¤.  

$$\begin{align*}
E(X) & = \sum_{x=0}^{\infty}xp(1 - p)^{x} = \frac{p(1 - p)}{p^{2}} = \frac{1 - p}{p} \\
\\
E(Y) & = E(X + 1) = \frac{1}{p}
\end{align*}$$

NumPyë¥¼ ì‚¬ìš©í•˜ë©´ ê¸°í•˜ ë¶„í¬í•˜ëŠ” í‘œë³¸ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆë‹¤.  

```python
import numpy as np

data = np.random.default_rng().geometric(p=0.35, size=1000)
```

## 6. ìŒì´í•­ ë¶„í¬

ì„±ê³µí•  í™•ë¥ ì´ $$p$$ì¸ ë² ë¥´ëˆ„ì´ ì‹œí–‰ì„ $$r$$ë²ˆ ì„±ê³µí•  ë•Œê¹Œì§€ ì‹œí–‰í•˜ëŠ” ê²½ìš°ì˜ ì‹¤íŒ¨(ì‹œí–‰) íšŸìˆ˜ì˜ ë¶„í¬ë¥¼ **ìŒì´í•­ ë¶„í¬(negative binomial distribution)**ë¼ í•˜ê³  ì•„ë˜ì™€ ê°™ì´ í‘œê¸°í•œë‹¤.

$$Y \sim NB(r, p)$$

ìŒì´í•­ ë¶„í¬ì—ì„œ ì‹œí–‰ íšŸìˆ˜ $$Y$$ì™€ ì‹¤íŒ¨ íšŸìˆ˜ $$X$$ì˜ ê´€ê³„ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.  

$$Y = X + r$$

ë”°ë¼ì„œ ìŒì´í•­ ë¶„í¬ë¥¼ ì‹œí–‰ íšŸìˆ˜ $$Y$$ì— ëŒ€í•œ ë¶„í¬ë¡œ ìƒê°í•˜ë©´ $$y - 1$$ ë²ˆì§¸ ê¹Œì§€ì˜ ê²°ê³¼ëŠ” $$r - 1$$ë²ˆ ì„±ê³µí•˜ëŠ” [ì´í•­ ë¶„í¬](#2-ì´í•­-ë¶„í¬)ì™€ ê°™ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ 1ë²ˆ ì„±ê³µí•˜ëŠ” ë¶„í¬ì´ë©°, ì‹¤íŒ¨ íšŸìˆ˜ $$X$$ì— ëŒ€í•œ ë¶„í¬ë¡œ ìƒê°í•˜ë©´ $$x + r - 1$$ ë²ˆì§¸ ê¹Œì§€ì˜ ê²°ê³¼ëŠ” $$r - 1$$ë²ˆ ì„±ê³µí•˜ê³  $$x$$ ë²ˆ ì‹¤íŒ¨í•œ [ì´í•­ ë¶„í¬](#2-ì´í•­-ë¶„í¬)ì™€ ê°™ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ 1ë²ˆ ì„±ê³µí•˜ëŠ” ë¶„í¬ì´ë‹¤.  

ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìŒì´í•­ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ ì •ë¦¬í•˜ë©´ ê°ê° ì•„ë˜ì™€ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.  

$$\begin{align*}
f_{Y}(y) & = \binom{y - 1}{r - 1}p^{r - 1}(1 - p)^{y - r}p, \quad y = r, r + 1, \cdots \\
\\
& = \binom{y - 1}{r - 1}p^{r}(1 - p)^{y - r} \\
\\
f(x) & = \binom{x + r - 1}{r - 1}p^{r - 1}(1 - p)^{x}p, \quad x = 0, 1, 2, \cdots \\
\\
& = \binom{x + r - 1}{r - 1}p^{r}(1 - p)^{x}
\end{align*}$$

ìŒì´í•­ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ Pythonìœ¼ë¡œ êµ¬í˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.  

```python
def nbinom_d(x: int, r: int, p: float) -> float:
    """
    returns probability of negative binomial distribution
    x: number of failures
    r: number of success
    p: probability
    """

    return combination(x + r - 1, r - 1) * (p ** r) * ((1 - p) ** x)


def nbinom_c(x: int, r: int, p: float, start: int = 0) -> float:
    """
    returns probability of negative binomial distribution
    x: number of failures
    r: number of success
    p: probability
    """

    return sum(nbinom_d(i, r, p) for i in range(start, x + 1))
```

SciPyë¥¼ ì‚¬ìš©í•˜ë©´ ìŒì´í•­ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ ê²°ê³¼ê°’ì„ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤.  

```python
from scipy.stats import nbinom

print(nbinom.pmf(k=4, n=3, p=0.3))
print(nbinom.cdf(k=4, n=3, p=0.3))
```

ìŒì´í•­ ë¶„í¬ì—ì„œ ê° ì‹œí–‰ì€ ê¸°í•˜ ë¶„í¬ë¥¼ ë”°ë¥´ê¸° ë•Œë¬¸ì— ìŒì´í•­ ë¶„í¬ì˜ ê¸°ëŒ€ê°’ì€ ì•„ë˜ì™€ ê°™ì´ ê¸°í•˜ ë¶„í¬ í™•ë¥ ë³€ìˆ˜ì˜ ê²°í•©ì„ í†µí•´ ìœ ë„í•  ìˆ˜ ìˆë‹¤.  

$$\begin{align*}
E(X) & = E(X_{1} + \cdots + X_{r}) = r\frac{1 - p}{p} \\
\\
E(Y) & = E(Y_{1} + \cdots + Y_{r}) = \frac{r}{p}
\end{align*}$$

NumPyë¥¼ ì‚¬ìš©í•˜ë©´ ìŒì´í•­ ë¶„í¬í•˜ëŠ” í‘œë³¸ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆë‹¤.  

```python
import numpy as np

data = np.random.default_rng().negative_binomial(n=1, p=0.1, size=100000)
```

## 7. ë‹¤í•­ ë¶„í¬

ê° ì‹œí–‰ì—ì„œ ë°œìƒ ê°€ëŠ¥í•œ ê²°ê³¼ê°€ $$k$$ ê°œ ì¼ ë•Œ, ê° ì‹œí–‰ì—ì„œ $$i$$ ë²ˆì§¸ ê²°ê³¼ì˜ í™•ë¥ ì€ $$p_{i}$$ë¡œ ê³ ì •ì´ê³  ê° ì‹œí–‰ì´ ë…ë¦½ì¸ ê²½ìš°ì˜ ë¶„í¬ë¥¼ **ë‹¤í•­ ë¶„í¬(multinomial distribution)**ë¼ í•œë‹¤. ì¦‰ [ì´í•­ ë¶„í¬](#2-ì´í•­-ë¶„í¬)ì—ì„œ ë°œìƒ ê°€ëŠ¥í•œ ì‚¬ê±´ì˜ ì¢…ë¥˜ê°€ $$k$$ê°œë¡œ ëŠ˜ì–´ë‚œ ê²½ìš°ë¥¼ ë§í•œë‹¤.  

ë”°ë¼ì„œ ë‹¤í•­ ë¶„í¬ëŠ” ì´í•­ ë¶„í¬ê°€ ê²°í•©ëœ í™•ë¥  ë³€ìˆ˜ì´ê¸° ë•Œë¬¸ì— ì´í•­ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤í•­ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ ì•„ë˜ì™€ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.  

$$\begin{gathered}
f(x_{1}, x_{2}, \cdots, x_{k}) = \frac{n!}{x_{1}!x_{2}! \cdots x_{k}!}P_{1}^{x_{1}}P_{2}^{x_{2}} \cdots P_{k}^{x_{k}} \\
\\ \sum_{i=1}^{k}x_{i} = n, \ \sum_{i=1}^{k}p_{i} = 1
\end{gathered}$$

ë‹¤í•­ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ Pythonìœ¼ë¡œ êµ¬í˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.  

```python
def multi_d(x: list[int], p: list[float]) -> float:
    """
    returns probability of multinomial distribution
    x: cases
    p: probability of each case
    """

    return factorial(sum(x)) / production(factorial(v) for v in x) * production(p ** x for p, x in zip(p, x))  # type: ignore
```

SciPyë¥¼ ì‚¬ìš©í•˜ë©´ ë‹¤í•­ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤.  

```python
from scipy.stats import multinomial

print(multinomial.pmf(x=[5, 6, 9], n=20, p=[0.3, 0.4, 0.3]))
```

ë‹¤í•­ ë¶„í¬ì—ì„œ $$i$$ ë²ˆì§¸ ì‹œí–‰ì˜ íŠ¹ì • ê²°ê³¼($$R_{i}$$)ì—ë§Œ ê´€ì‹¬ì´ ìˆëŠ” ê²½ìš°ì— ëŒ€í•œ ê¸°ëŒ€ê°’ì€ ì•„ë˜ì™€ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.  

$$\begin{align*}
X_{i} \sim B(n, p_{i}) \ \to \ E(X_{i}) & = np_{i} \\
\\
Var(X_{i}) & = np_{i}(1 - p_{i})
\end{align*}$$

ë˜í•œ íŠ¹ì • ê²°ê³¼ë“¤($$R_{i} \cup R_{j}$$)ì— ê´€ì‹¬ì´ ìˆëŠ” ê²½ìš° ì•„ë˜ì™€ ê°™ì´ ì´í•­ ë¶„í¬ì˜ [ê²°í•© ë¶„í¬](/statistics/statistics_09/#2-ê³µë¶„ì‚°ê³¼-ìƒê´€ê³„ìˆ˜)ë¡œ ë‹¤ë£° ìˆ˜ ìˆë‹¤.  

$$\begin{align*}
Y_{X_{i} + X_{j}} \sim B(n, p_{i} + p_{j}) \ \to \ E(Y) & = E(X_{i} + X_{j}) = n(p_{i} + p_{j}) \\
\\
Var(Y) & = Var(X_{i} + X_{j}) = Var(X) + Var(Y) \pm 2Cov(X, Y) \\
\\
& = np_{i}(1 - p_{i}) + np_{j}(1 - p_{j}) - 2np_{i}p_{j} = n(p_{i} + p_{j} - (p_{i} + p_{j})^{2}) \\
\\
& = n(p_{i} + p_{j})(1 - (p_{i} + p_{j}))
\end{align*}$$

ë‹¤í•­ ë¶„í¬ì˜ ê³µë¶„ì‚°ê³¼ ìƒê´€ê³„ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.  

$$\begin{align*}
Cov(X_{i}, X_{j}) & = -np_{i}p_{j} \\
\\
Cor(X_{i}, X_{j}) & = \frac{-np_{i}p_{j}}{\sqrt{np_{i}(1 - p_{i})}\sqrt{np_{j}(1 - p_{j})}} \\
\\
& = -\sqrt{\frac{p_{i}p_{j}}{(1 - p_{i})(1 - p_{j})}}
\end{align*}$$

<details><summary>ë‹¤í•­ ë¶„í¬ì˜ ê³µë¶„ì‚° ìœ ë„</summary><div markdown="1">

í†µê³„í•™ì—ì„œ ê²°í•© ë¶„í¬ì˜ ìƒê´€ê³„ìˆ˜ë¥¼ êµ¬í•  ë•ŒëŠ” ê°ê°ì˜ ê²°í•©ì— ëŒ€í•œ ëª¨ë“  ê²½ìš°ì˜ ìƒê´€ê³„ìˆ˜ë¥¼ êµ¬í•œ í›„ì— ì „ë¶€ ë”í•˜ë©´ ëœë‹¤.  

ë”°ë¼ì„œ ë‹¤í•­ ë¶„í¬ì—ì„œ $$i$$ ë²ˆì§¸ ë²”ì£¼ì˜ ë°œìƒ ë¹ˆë„ì™€ $$j$$ ë²ˆì§¸ ë²”ì£¼ì˜ ë°œìƒ ë¹ˆë„, ì¦‰ $$X_{i}$$ì™€ $$X_{j}$$ì˜ ê´€ê³„ë¥¼ ìƒê´€ê³„ìˆ˜ë¥¼ í†µí•´ í™•ì¸í•˜ëŠ” ë°©ë²•ì„ 2íšŒ ì‹¤í—˜í•œ ê²½ìš°ë¥¼ í†µí•´ ìœ ë„í•˜ë©´ ê³µë¶„ì‚°ì€ ì•„ë˜ì™€ ê°™ì´ êµ¬í•  ìˆ˜ ìˆë‹¤.  

$$Cov(X_{11} + X_{21}, X_{12} + X_{22}) = Cov(X_{11}, X_{12}) + Cov(X_{11}, X_{22}) + Cov(X_{21}, X_{12}) + Cov(X_{21}, X_{22})$$

ì´ ë•Œ ë³„ê°œì˜ ì‹¤í–‰ì¸ ê²½ìš°ëŠ” ë…ë¦½ì¸ ì‚¬ê±´ì´ê¸° ë•Œë¬¸ì— ê³µë¶„ì‚°ì€ 0ì´ë©° ë”°ë¼ì„œ [ì´í•­ ë¶„í¬](#2-ì´í•­-ë¶„í¬)ì˜ ë¶„ì‚°ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.  

$$\begin{align*}
Cov(X_{1}, X_{2}) & = \sum_{i=1}^{n}Cov(X_{i1}, X_{i2}) \Rightarrow -np_{i}p_{j} \\
\\
Cov(X_{i1}, X_{i2}) & = E(X_{i1}X_{i2}) - E(X_{i1})E(X_{i2}) \\
\\
& = -p_{1}p_{2} \quad \because E(X_{ij}) = p_{j}, \ E(X_{i1}X_{i2}) = 0
\end{align*}$$

</div></details><br>

ğŸ’¡ìœ„ ì‹ì—ì„œ ì„±ê³µ í™•ë¥ ê³¼ ì‹¤íŒ¨ í™•ë¥ ì˜ ë¹„ìœ¨ì¸ $$p_{i}/(1 - p_{i})$$ë¥¼ **ì˜¤ì¦ˆ(odds)**ë¼ í•œë‹¤.  
{:.note}

NumPyë¥¼ ì‚¬ìš©í•˜ë©´ ë‹¤í•­ ë¶„í¬í•˜ëŠ” í‘œë³¸ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆë‹¤.  

```python
import numpy as np

data = np.random.default_rng().multinomial(n=1, p=0.1, size=100000)
```

---
## Reference
- [êµ¬í˜„í•œ í•¨ìˆ˜ git repository](https://github.com/djccnt15/mathematics)