---
published: true
layout: post
title: '[ê¸°ì´ˆí†µê³„í•™] 10. ë‹¤ì–‘í•œ ì´ì‚°í™•ë¥ ë¶„í¬'
description: >
    ë² ë¥´ëˆ„ì´ ë¶„í¬, ì´í•­ ë¶„í¬, ì´ˆê¸°í•˜ ë¶„í¬, í¬ì•„ì†¡ ë¶„í¬, ê¸°í•˜ ë¶„í¬
categories: [Statistics]
tags: [statistics]
image:
    path: /assets/img/posts/thumbnail_statistics_10.png
related_posts:
    - _posts/statistics/2023-01-01-statistics_09.md
---
{% include series_statistics.html %}
* toc
{:toc}

## 1. ë² ë¥´ëˆ„ì´ ë¶„í¬

ë‹¤ìŒì˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì‹¤í—˜ì„ **ë² ë¥´ëˆ„ì´ ì‹œí–‰(Bernoulli trial)**ì´ë¼ í•œë‹¤.  

- ê° ì‹¤í—˜ì—ì„œ ë°œìƒ ê°€ëŠ¥í•œ ê²°ê³¼ê°€ ë‹¨ ë‘ ê°€ì§€
- ê° ì‹¤í—˜ì€ ë…ë¦½ì ìœ¼ë¡œ ìˆ˜í–‰
- ëª¨ë“  ì‹¤í—˜ì—ì„œ ê²°ê³¼ì˜ í™•ë¥ ì€ í•­ìƒ ë™ì¼

ğŸ’¡ ëª¨ì§‘ë‹¨ì´ ì¶©ë¶„íˆ í¬ê³  í‘œë³¸í¬ê¸°ê°€ ìƒëŒ€ì ìœ¼ë¡œ í¬ì§€ ì•Šì€ ê²½ìš° ë¹„ë³µì›ì¶”ì¶œë„ ë² ë¥´ëˆ„ì´ ì‹¤í—˜ì„ ê·¼ì‚¬ëª¨í˜•ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤.  
{:.note}

ëª¨ìˆ˜(parameter)ì¸ ì„±ê³µ í™•ë¥ ì´ $$p$$ì¸ ë² ë¥´ëˆ„ì´ ì‹œí–‰ì˜ í™•ë¥ ë³€ìˆ˜ì˜ ë¶„í¬ë¥¼ **ë² ë¥´ëˆ„ì´ ë¶„í¬(Bernoulli distribution)**ë¼ í•˜ê³ , ì•„ë˜ì™€ ê°™ì´ í‘œê¸°í•œë‹¤.  

$$X \sim B(p)$$

ë² ë¥´ëˆ„ì´ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ì˜ ì¼ë°˜ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.  

$$f(x) = P(X = x) = p^{x}(1 - p)^{1 - x}, \quad x = 0, 1$$

ë² ë¥´ëˆ„ì´ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ Pythonìœ¼ë¡œ êµ¬í˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.  

```python
def bernoulli_d(x: int, p: float, n: int = 1) -> float:
    """
    returns probability of bernoulli distribution
    x: case
    p: probability
    """

    res = (p ** x) * ((1 - p) ** (n - x))
    return res
```

ë² ë¥´ëˆ„ì´ ë¶„í¬ì˜ ê¸°ëŒ€ê°’ê³¼ ë¶„ì‚°ì€ ì•„ë˜ì™€ ê°™ë‹¤.  

$$\begin{align*}
& E(X) = 0 \times (1 - p) + 1 \times p = p \\
\\
& E(X^{2}) = 0^{2} \times (1 - p) + 1^{2} \times p = p \\
\\
& Var(X) = p - p^{2} = p(1 - p) \\
\\
& SD(X) = \sqrt{p(1 - p)}
\end{align*}$$

## 2. ì´í•­ ë¶„í¬

ì„±ê³µ í™•ë¥ ì´ $$p$$ì¸ ë² ë¥´ëˆ„ì´ ì‹¤í—˜ì„ $$n$$ë²ˆ ë°˜ë³µí–ˆì„ ë•Œ, ì„±ê³µ íšŸìˆ˜ $$X$$ì˜ ë¶„í¬ë¥¼ **ì´í•­ ë¶„í¬(binomial distribution)**ë¼ í•œë‹¤.  

$$X_{i} \sim B(p)$$ë¼ê³  í•  ë•Œ, ì„±ê³µ íšŸìˆ˜ $$X$$ëŠ” $$n$$ê°œì˜ ë² ë¥´ëˆ„ì´ í™•ë¥ ë³€ìˆ˜ì˜ í•©ìœ¼ë¡œ í‘œì‹œí•œë‹¤.  

$$X = X_{1} + X_{2} + \cdots + X_{n}$$

ë”°ë¼ì„œ [ë…ë¦½ì¸ ê²°í•© ë¶„í¬ì˜ ì„±ì§ˆ](/statistics/statistics_09/#2-ê³µë¶„ì‚°ê³¼-ìƒê´€ê³„ìˆ˜)ì„ ë°”íƒ•ìœ¼ë¡œ ì´í•­ ë¶„í¬ì˜ ê¸°ëŒ€ê°’ê³¼ ë¶„ì‚°ì„ ìœ ë„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.  

$$\begin{align*}
E(X_{i}) = p \ & \to \ E(X) = np \\
\\
Var(X_{i}) = p(1 - p) \ & \to \ Var(X) = np(1 - p) \\
\\
SD(X_{i}) = \sqrt{p(1 - p)} \ & \to \ SD(X) = \sqrt{np(1 - p)}
\end{align*}$$

ì‹œí–‰ íšŸìˆ˜ë¥¼ $$n$$, ì„±ê³µ í™•ë¥  $$p$$ì¸ ì´í•­ ë¶„í¬ë¥¼ ì•„ë˜ì™€ ê°™ì´ í‘œê¸°í•œë‹¤.  

$$X \sim B(n, p)$$

ì´í•­ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ì˜ ì¼ë°˜ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.  

$$f(x) = \binom{n}{x}p^{x}(1 - p)^{n - x}, \quad x = 0, 1, \cdots, n$$

ì´í•­ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ Pythonìœ¼ë¡œ êµ¬í˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.  

```python
def binom_d(x: int, n: int, p: float) -> float:
    """
    returns probability of binom distribution
    x: case
    n: number of trial
    p: probability
    """

    res = combination(n, x) * bernoulli_d(x=x, n=n, p=p)
    return res


def binom_c(x: int, n: int, p: float) -> float:
    """
    returns cumulative probability of binom distribution
    x: case
    n: number of trial
    p: probability
    """

    res = sum(binom_d(i, n, p) for i in range(x + 1))
    return res
```

$$X \sim B(m, p), Y \sim B(n, p)$$ì´ê³  $$X, Y$$ê°€ ë…ë¦½ì¸ ê²½ìš° ì´í•­ ë¶„í¬ì˜ ê²°í•©ì€ ì•„ë˜ì™€ ê°™ë‹¤.  

$$X + Y \sim B(m + n, p)$$

ì•„ë˜ì™€ ê°™ì´ NumPyë¥¼ ì‚¬ìš©í•˜ë©´ ì´í•­ ë¶„í¬í•˜ëŠ” í‘œë³¸ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆë‹¤.  

```python
import numpy as np

n = 10      # number of trials
p = 0.5     # probability of each trial
size = 100  # size of data
data = np.random.default_rng(seed=0).binomial(n=n, p=p, size=size)
```

## 3. ì´ˆê¸°í•˜ ë¶„í¬

ê° ì‹¤í—˜ì—ì„œ ë°œìƒ ê°€ëŠ¥í•œ ê²°ê³¼ê°€ ë‹¨ ë‘ ê°€ì§€ì´ê³ , í¬ê¸°ê°€ $$N$$ì¸ ëª¨ì§‘ë‹¨(ìœ í•œëª¨ì§‘ë‹¨)ì´ ê°ê° $$M$$ê³¼ $$N - M$$ í¬ê¸°ì˜ ë¶€ëª¨ì§‘ë‹¨ $$A, B$$ë¡œ ë‚˜ë‰˜ì–´ì§„ ê²½ìš°ì—ì„œ $$n$$ê°œì˜ í‘œë³¸ì„ ë¬´ì‘ìœ„ë¡œ ë¹„ë³µì›ì¶”ì¶œí•  ë•Œ, ë¶€ëª¨ì§‘ë‹¨ $$A$$ì—ì„œ ì¶”ì¶œëœ í‘œë³¸ ìˆ˜ì˜ ë¶„í¬ë¥¼ **ì´ˆê¸°í•˜ ë¶„í¬(hypergeometric distribution)**ë¼ í•œë‹¤.  

ì´ˆê¸°í•˜ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ì˜ ì¼ë°˜ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.  

$$f(x) = \frac{\binom{M}{x}\binom{N - M}{n - x}}{\binom{N}{n}}, \quad x = max(0, n - N + M), \cdots, min(n, M)$$

ì´ˆê¸°í•˜ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ Pythonìœ¼ë¡œ êµ¬í˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.  

```python
def hyper_d(x: int, M: int, n: int, N: int) -> float:
    """
    returns probability of hypergeometric distribution
    x: case
    M: size of subpopulation
    n: size of sample
    N: size of population
    """

    res = combination(M, x) * combination(N - M, n - x) / combination(N, n)
    return res


def hyper_c(x: int, M: int, n: int, N: int) -> float:
    """
    returns cumulative probability of hypergeometric distribution
    x: case
    M: size of subpopulation
    n: size of sample
    N: size of population
    """

    res = sum(combination(M, x) * combination(N - M, n - x) / combination(N, n) for x in range(x + 1))
    return res
```

$$N$$ì´ í¬ê³  $$N$$ì— ë¹„í•´ $$n$$ì´ ìƒëŒ€ì ìœ¼ë¡œ ë§¤ìš° ì‘ì€ ê²½ìš°($$n \ll N$$) ë¹„ë³µì›ì˜ íš¨ê³¼ê°€ ì ê¸° ë•Œë¬¸ì— ë² ë¥´ëˆ„ì´ ì‹¤í—˜ìœ¼ë¡œ ê·¼ì‚¬í•˜ë©°, ë”°ë¼ì„œ ì´ˆê¸°í•˜ ë¶„í¬ ì—­ì‹œ $$p = M/N$$ì¸ ì´í•­ ë¶„í¬ë¡œ ê·¼ì‚¬í•œë‹¤.  

ì´ˆê¸°í•˜ ë¶„í¬ì˜ ê¸°ëŒ€ê°’ê³¼ ë¶„ì‚°ì€ ì•„ë˜ì™€ ê°™ë‹¤.  

$$\begin{align*}
E(X) & = n \frac{M}{N} = np \\
\\
Var(X) & = np(1 - p) - n(n - 1)\frac{p(1 - p)}{N - 1} \\
& = np(1 - p)\frac{N - n}{N - 1} = n\frac{M}{N} \left( 1 - \frac{M}{N} \right) \leq np(1 - p)
\end{align*}$$

<details><summary>ì´ˆê¸°í•˜ ë¶„í¬ ê¸°ëŒ€ê°’ê³¼ ë¶„ì‚°ì˜ ìœ ë„</summary><div markdown="1">

$$E(X_{i}) = \frac{M}{N} = p \ \to \ E(X) = n \frac{M}{N} = np$$

$$\begin{align*}
E(X_{i}) & = \frac{M}{N} = p, \quad E(X_{i}^{2}) = \frac{M}{N} = p \\
\\
\therefore Var(X_{i}) & = p - p^{2} = p(1 - p) = \frac{M}{N}\frac{N - M}{N} \\
\\
\therefore Var(X) & = \sum_{i}Var(X_{i}) + 2\sum_{i < j}Cov(X_{i}, X_{j}) \\
\\
Cov(X_{i}, X_{j}) & = E(X_{i}X_{j}) - E(X_{i})E(X_{j}) \\
\\
E(X_{i}X_{j}) & = P(X_{i} = 1, X_{j} = 1) \quad \because X_{i} = 0 \ \to \ E(X_{i}X_{j}) = 0 \\
& = P(X_{i} = 1)P({X_{j} = 1 \vert X_{i} = 1}) = \frac{M}{N}\frac{M - 1}{N - 1} \\
\\
\therefore Cov(X_{i}, X_{j}) & = \frac{M}{N}\frac{M - 1}{N - 1} - \left( \frac{M}{N} \right)^{2} \\
& = -\frac{M}{N}\frac{N - M}{N(N - 1)} = -\frac{p(1 - p)}{N - 1} \leq 0 \\
\\
\therefore Var(X) & = np(1 - p) - n(n - 1)\frac{p(1 - p)}{N - 1} \\
& = np(1 - p)\frac{N - n}{N - 1}
\end{align*}$$

</div></details><br>

ì´ ë•Œ ìœ„ ì‹ì—ì„œ $$\frac{N - n}{N - 1}$$ì„ ìœ í•œëª¨ì§‘ë‹¨ ìˆ˜ì •ê³„ìˆ˜ë¼ í•œë‹¤.  

ìœ í•œëª¨ì§‘ë‹¨ìœ¼ë¡œë¶€í„° ë¹„ë³µì›ì¶”ì¶œì„ í•˜ë©´ì„œ ë¶„ì‚°ì´ ì‘ì•„ì§„ë‹¤ëŠ” ê²ƒì€ í¼ì ¸ìˆëŠ” ì •ë„ê°€ ì‘ì•„ì ¸ ë°ì´í„°ì˜ ë³€ë™ì„±ì´ ì ì–´ì§„ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ìˆ˜ë¥¼ ì¶”ì •í–ˆì„ ë•Œ ë” ì•ˆì •ì ì¸ í˜•íƒœë¥¼ ê°–ëŠ”ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.  

ì•„ë˜ì™€ ê°™ì´ NumPyë¥¼ ì‚¬ìš©í•˜ë©´ ì´ˆê¸°í•˜ ë¶„í¬í•˜ëŠ” í‘œë³¸ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆë‹¤.  

```python
import numpy as np

data = np.random.default_rng(seed=0).hypergeometric(ngood=20, nbad=20, nsample=10, size=100)
```

## 4. í¬ì•„ì†¡ ë¶„í¬

[ì´í•­ ë¶„í¬](#2-ì´í•­ ë¶„í¬)ì—ì„œ $$n$$ì´ ë§¤ìš° ì»¤ì§€ë©´ ê³„ì‚°ì— ì–´ë ¤ì›€ì´ ìƒê¸°ëŠ”ë°, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í¬ì•„ì†¡ ë¶„í¬ë¥¼ ì‚¬ìš©í•œë‹¤.  

í™•ë¥ ë³€ìˆ˜ $$X$$ê°€ ì´í•­ ë¶„í¬ $$B(n, p)$$ë¥¼ ë”°ë¥¼ ë•Œ $$p$$ê°€ ë§¤ìš° ì‘ìœ¼ë©´ í° $$x$$ì— ëŒ€í•œ í™•ë¥ ì€ ë¬´ì‹œí•  ì •ë„ë¡œ ì‘ì•„ì§€ëŠ”ë°, ì´ ê²½ìš°ì˜ í™•ë¥ ë¶„í¬ë¥¼ **í¬ì•„ì†¡ ë¶„í¬(Poisson distribution)**ë¼ í•˜ë©° ì•„ë˜ì™€ ê°™ì´ í‘œê¸°í•œë‹¤.  

$$X \sim \text{Pois}(\lambda)$$

ì¦‰, ë°œìƒ ê°€ëŠ¥ì„±ì´ í¬ë°•í•œ ì‚¬ê±´ì´ ì„ì˜ì˜ êµ¬ê°„ì—ì„œ í‰ê· ì ìœ¼ë¡œ $$\lambda$$ë²ˆ ë°œìƒí•˜ëŠ” ìƒí™©ì—ì„œ, êµ¬ê°„ì„ ë‚˜ëˆ„ì—ˆì„ ë•Œ ê° êµ¬ê°„ì˜ ë°œìƒ ë¹ˆë„ëŠ” ì„œë¡œ ë…ë¦½(independent increment)ì´ê³  êµ¬ê°„ì˜ ìœ„ì¹˜ì™€ ê´€ê³„ì—†ì´ ë™ì¼ ê¸¸ì´ì˜ êµ¬ê°„ì—ì„œì˜ í‰ê· ë°œìƒ ë¹ˆë„ëŠ” ë™ì¼(stationary increment)í•˜ë©´ í•´ë‹¹ ë¶„í¬ëŠ” í¬ì•„ì†¡ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.  

í¬ì•„ì†¡ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.  

$$f(x) = \binom{n}{x}p^{x}(1 - p)^{n - x} \simeq \frac{e^{-\lambda}\lambda^{x}}{x!}, \quad \lambda = np = E(X)$$

<details><summary>í¬ì•„ì†¡ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ ìœ ë„</summary><div markdown="1"><br>

ì´í•­ ë¶„í¬í•˜ëŠ” í™•ë¥ ë³€ìˆ˜ $$X$$ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ $$\lambda$$ë¥¼ ì´ìš©í•´ì„œ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.  

$$\begin{gathered}
E(X) = \lambda = np \ \to \ p = \frac{\lambda}{n} \\
\\
\Rightarrow f(x) = \binom{n}{x}p^{x}(1 - p)^{n - x} = \frac{n!}{x!(n - x)!} \left( \frac{\lambda}{n} \right)^{x} \left( 1 - \frac{\lambda}{n} \right)^{n - x}
\end{gathered}$$

ìœ„ ì‹ì—ì„œ $$n$$ì´ ë¬´í•œëŒ€ë¡œ ë°œì‚°í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ì •ë¦¬ëœë‹¤.  

$$\begin{gathered}
\frac{n!}{(n - x)!n^{x}} = \frac{n(n - 1) \cdots (n - x + 1)}{n^{x}} \ \to \ 1 \\
\\
\lim_{n \to \infty} \left( 1 - \frac{\lambda}{n} \right)^{n} = e^{-\lambda}, \quad \because \lim_{n \to \infty} \left( 1 + \frac{x}{n} \right)^{n} = e^{x} \\
\\
\lim_{n \to \infty} \left( 1 - \frac{\lambda}{n} \right)^{-x} = 1 \\
\\
\Rightarrow f(x) = \binom{n}{x}p^{x}(1 - p)^{n - x} \simeq \frac{e^{-\lambda}\lambda^{x}}{x!}
\end{gathered}$$

</div></details><br>

ğŸ’¡ $$p$$ê°€ ì»¤ì§ˆìˆ˜ë¡ í¬ì•„ì†¡ ê·¼ì‚¬ì™€ ì´í•­ë¶„í¬ì˜ ì˜¤ì°¨ê°€ ì»¤ì§€ëŠ”ë°, ì¼ë°˜ì ìœ¼ë¡œ $$\lambda$$ ê°’ì´ 5ë³´ë‹¤ ì‘ìœ¼ë©´ í¬ì•„ì†¡ ê·¼ì‚¬ë¥¼ ì‚¬ìš©í•´ë„ í° ë¬¸ì œê°€ ì—†ë‹¤ê³  í•œë‹¤.  
{:.note}

í¬ì•„ì†¡ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ Pythonìœ¼ë¡œ êµ¬í˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.  

```python
import math


def pois_d(x: int, l: float) -> float:
    """
    returns probability of poisson distribution
    x: case
    l: lambda, expectation of random variable
    """

    res = (math.e ** -l) * (l ** x) / factorial(x)
    return res


def pois_c(x: int, l: float) -> float:
    """
    returns cumulative probability of poisson distribution
    x: case
    l: lambda, expectation of random variable
    """

    res = sum(pois_d(i, l) for i in range(x + 1))
    return res
```

$$X \sim \text{Pois}(\lambda_{1}), Y \sim \text{Pois}(\lambda_{2})$$ì´ê³ , $$X, Y$$ê°€ ë…ë¦½ì¸ ê²½ìš° í¬ì•„ì†¡ ë¶„í¬ì˜ ê²°í•©ì€ ì•„ë˜ì™€ ê°™ë‹¤.  

$$X + Y \sim \text{Pois}(\lambda_{1} + \lambda_{2})$$

í¬ì•„ì†¡ ë¶„í¬ì˜ ê¸°ëŒ€ê°’ê³¼ ë¶„ì‚°ì€ ì•„ë˜ì™€ ê°™ë‹¤.  

$$\begin{align*}
E(X) & = \lambda \\
\\
Var(X) & = \lambda
\end{align*}$$

<details><summary>í¬ì•„ì†¡ ë¶„í¬ ê¸°ëŒ€ê°’ê³¼ ë¶„ì‚°ì˜ ìœ ë„</summary><div markdown="1">

$$\begin{align*}
E(X) & = \sum_{x=0}^{\infty}x\frac{e^{-\lambda}\lambda^{x}}{x!} = \sum_{x=1}^{\infty}x\frac{e^{-\lambda}\lambda^{x}}{x!} \\
\\
& = \lambda\sum_{x=1}^{\infty}\frac{e^{-\lambda}\lambda^{x - 1}}{(x - 1)!} \\
\\
y = x - 1 & \ \to \ \lambda\sum_{x=1}^{\infty}\frac{e^{-\lambda}\lambda^{x - 1}}{(x - 1)!} = \lambda\sum_{y=0}^{\infty}\frac{e^{-\lambda}\lambda^{y}}{y!} = \lambda
\end{align*}$$

$$\begin{align*}
Var(X) & = E(X^{2}) - E(X)^{2} \\
\\
& = E(X(X - 1)) + E(X) - E(X)^{2} \\
\\
& = \lambda^{2} + \lambda - \lambda^{2} = \lambda \\
\\
\because E(X(X - 1)) & = E(X^{2}) - E(X) \\
\\
& = \sum_{x=0}^{\infty}x(x - 1)\frac{e^{-\lambda}\lambda^{x}}{x!} = \lambda^{2}\sum_{x=2}^{\infty}\frac{e^{-\lambda}\lambda^{x - 2}}{(x - 2)!} \\
\\
y = x - 2 & \ \to \ \lambda^{2}\sum_{y=0}^{\infty}\frac{e^{-\lambda}\lambda^{y}}{y!} = \lambda^{2}
\end{align*}$$

</div></details><br>

ğŸ’¡ ë”°ë¼ì„œ ëª¨ì§‘ë‹¨ì´ í¬ì•„ì†¡ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ë©´, ê¸°ëŒ€ê°’ê³¼ ë¶„ì‚°ì´ ë¹„ìŠ·í•œ ê°’ì„ ê°€ì§„ë‹¤ê³  ì˜ˆìƒí•  ìˆ˜ ìˆìœ¼ë©°,  
â— í‰ê· ì— ë¹„í•´ í‘œë³¸ë¶„ì‚°ì´ ë§¤ìš° í¬ë‹¤ë©´($$\overline{x} \ll s^{2}$$) ë°ì´í„°ê°€ í¬ì•„ì†¡ ë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•Šì„ ê²ƒì´ë¼ê³  ì˜ˆìƒí•  ìˆ˜ ìˆë‹¤.  
{:.note}

ì•„ë˜ì™€ ê°™ì´ NumPyë¥¼ ì‚¬ìš©í•˜ë©´ í¬ì•„ì†¡ ë¶„í¬í•˜ëŠ” í‘œë³¸ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆë‹¤.  

```python
import numpy as np

data = np.random.default_rng().poisson(lam=5, size=1000)
```

## 5. ê¸°í•˜ ë¶„í¬

ì„±ê³µ í™•ë¥ ì´ $$p$$ì¸ [ë² ë¥´ëˆ„ì´ ì‹œí–‰](#1-ë² ë¥´ëˆ„ì´-ë¶„í¬)ì„ ì„±ê³µí•  ë•Œê¹Œì§€ ì‹œí–‰í•˜ëŠ” ê²½ìš° ì‹¤íŒ¨(ì‹œí–‰) íšŸìˆ˜ì˜ ë¶„í¬ë¥¼ **ê¸°í•˜ ë¶„í¬(geometric distribution)**ë¼ í•˜ê³ , ì•„ë˜ì™€ ê°™ì´ í‘œê¸°í•œë‹¤.  

$$X \sim \text{Geo}(p)$$

ê¸°í•˜ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.  

$$f(x) = (1 - p)^{x}p, \quad x = 0, 1, 2, \cdots$$

ìœ„ì™€ ê°™ì´ 1í•­ì´ $$p$$ì´ê³  ê³µë¹„ê°€ $$1 - p$$ì¸ ë“±ë¹„ê¸‰ìˆ˜ í˜•íƒœë¥¼ ê°–ê³  ìˆê¸° ë•Œë¬¸ì— $$x$$ë²ˆì§¸ ì‹¤í—˜ ì´ì „ì— ì„±ê³µí•  í™•ë¥ ì„ ì˜ë¯¸í•˜ëŠ” ê¸°í•˜ ë¶„í¬ì˜ ëˆ„ì ë¶„í¬í•¨ìˆ˜ëŠ” ë“±ë¹„ê¸‰ìˆ˜ì˜ í•©ì„ ê¸°ë°˜ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì´ ìœ ë„í•  ìˆ˜ ìˆë‹¤.  

$$\begin{align*}
P(X \leq x) & = \sum_{k=0}^{x}p(1 - p)^{k} = \frac{p - p(1 - p)^{x + 1}}{1 - (1 - p)} = 1 - (1 - p)^{x + 1} \\
\\
P(X \geq x) & = 1 - P(X \leq x - 1) = (1 - p)^{x}
\end{align*}$$

ê¸°í•˜ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ Pythonìœ¼ë¡œ êµ¬í˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.  

```python
def geom_d(x: int, p: float) -> float:
    """
    returns probability of geometric distribution
    x: number of failures
    p: probability
    """

    res = ((1 - p) ** x) * p
    return res


def geom_c(x: int, p: float) -> float:
    """
    returns cumulative probability of geometric distribution
    x: number of failures
    p: probability
    """

    res = 1 - ((1 - p) ** (x + 1))
    return res
```

ê¸°í•˜ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ ì‹œí–‰ íšŸìˆ˜ì— ëŒ€í•œ ì‹ìœ¼ë¡œ ë³€í™˜í•˜ë©´, ì‹œí–‰ íšŸìˆ˜ $$Y$$ëŠ” ì‹¤íŒ¨ íšŸìˆ˜ $$X + 1$$ê³¼ ë™ì¼í•˜ê¸° ë•Œë¬¸ì— ì•„ë˜ì™€ ê°™ì´ ë³€í˜•í•  ìˆ˜ ìˆë‹¤.  

$$Y = X + 1 \ \to \ f_{Y}(y) = (1 - p)^{y - 1}p, \quad y = 1, 2, \cdots$$

ë”°ë¼ì„œ $$x$$ë²ˆì§¸ ì‹¤í—˜ ì´ì „ì— ì„±ê³µí•  í™•ë¥ ì€ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.  

$$P(Y \leq x) = P(X \leq x - 1)$$

ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹œí–‰ íšŸìˆ˜ì— ëŒ€í•œ ëˆ„ì ë¶„í¬í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.  

$$\begin{align*}
P(Y \leq y) & = P(X + 1 \leq y) = P(X \leq y - 1) = 1 - (1 - p)^{y} \\
\\
P(Y > y) & = 1 - P(Y \leq y) = (1 - p)^{y}
\end{align*}$$

ê¸°í•˜ ë¶„í¬ì˜ ê¸°ëŒ€ê°’ì€ ë¬´í•œë“±ë¹„ê¸‰ìˆ˜ì˜ í•©ì„ ê¸°ë°˜ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì´ ìœ ë„í•  ìˆ˜ ìˆë‹¤.  

$$\begin{align*}
E(X) & = \sum_{x=0}^{\infty}xp(1 - p)^{x} = \frac{p(1 - p)}{p^{2}} = \frac{1 - p}{p} \\
\\
E(Y) & = E(X + 1) = \frac{1}{p}
\end{align*}$$

---
## Reference
- [êµ¬í˜„í•œ í•¨ìˆ˜ git repository](https://github.com/djccnt15/mathematics)