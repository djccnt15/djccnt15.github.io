---
published: true
layout: post
title: '[TextMining] TF-IDF'
description: >
    단어의 중요도를 파악하는 방법
categories: [DataAnalysis]
tags: [data analysis, text mining, konlpy]
image:
    path: /assets/img/posts/github_pages.png
related_posts:
    - _posts/data_science/2022-06-16-installing_konlpy.md
---
* toc
{:toc}

## 개요

단어의 등장 순서를 고려하지 않는 빈도수 기반 자연어 처리 모델을 **bag of words model(단어 가방 모형)**이라고 하는데, 이 bag of words 모델에서는 TF와 IDF를 결합한 TF-IDF값을 통해 단어의 중요도를 파악한다.  

## TF

**TF(Term Frequency, 단어 빈도)**는 단어에서 유추할 수 있듯이 특정한 단어가 문서 내에 얼마나 자주 등장하는지를 나타내는 값으로, 기본적으로 이 값이 높을수록 문서에서 중요하다고 판단할 수 있다. 아래 문장들에 대해 TF값을 계산해보자.  

- 중소 기업들의 디지털전환 전략
- 스마트공장 지원을 통한 제조 기업의 디지털전환
- 전체 제조 기업 중 중소 기업이 95% 이상을 차지함
- 전체 제조 기업 종사자 중 중소 기업 종사자가 70% 이상을 차지함

```python
from konlpy.tag import Okt

stop_words = ['은', '는', '이', '가', '의', '을', '를', '들']

okt = Okt()

def bag_of_words(doc):
    tokenized_doc = okt.morphs(doc) # tokenize doc by morpheme
    word_index = {}
    bow = []

    for word in tokenized_doc:
        if word in stop_words: # remove stop words
            continue

        elif word not in word_index.keys(): # add word to word_index
            word_index[word] = len(word_index)
            bow.insert(len(word_index) - 1, 1)

        else: # count word
            index = word_index.get(word)
            bow[index] += 1

    return word_index, bow

corpus = [
    '중소 기업들의 디지털전환 전략',
    '스마트공장 지원을 통한 제조 기업의 디지털전환',
    '전체 제조 기업 중 중소 기업이 95% 이상을 차지함',
    '전체 제조 기업 종사자 중 중소 기업 종사자가 70% 이상을 차지함',
]

for doc in corpus:
    vocab, bow = bag_of_words(doc)

    print(vocab)
    print(bow)
```
```
{'중소': 0, '기업': 1, '디지털': 2, '전환': 3, '전략': 4}
[1, 1, 1, 1, 1]
{'스마트': 0, '공장': 1, '지원': 2, '통한': 3, '제조': 4, '기업': 5, '디지털': 6, '전환': 7}
[1, 1, 1, 1, 1, 1, 1, 1]
{'전체': 0, '제조': 1, '기업': 2, '중': 3, '중소': 4, '95%': 5, '이상': 6, '차지': 7, '함': 8}
[1, 1, 2, 1, 1, 1, 1, 1, 1]
{'전체': 0, '제조': 1, '기업': 2, '종사': 3, '자': 4, '중': 5, '중소': 6, '70%': 7, '이상': 8, '차지': 9, '함': 10}
```

## IDF

단어의 중요도는 기본적으로 TF를 통해서 파악할 수 있다. 그러나 영어의 관사나 한국어의 조사와 같이 문서군 내에서 매우 자주 사용 되는 단어가 오히려 전혀 중요하지 않은 단어일 가능성이 있는데, 따라서 단어의 중요도 가중치를 보정해주기 위한 방법이 필요하다. 이 때 사용되는 방법이 **DF(Document Frequency, 문서 빈도)**로, 특정 단어가 몇 개의 문서에 등장했는지에 대한 정보이다.  

TF의 가중치 보정을 위해 DF에 역수를 취한 것이 **[IDF(Inverse Document Frequency)](https://www.researchgate.net/publication/238123710_Understanding_Inverse_Document_Frequency_On_Theoretical_Arguments_for_IDF)**로, IDF의 계산에는 여러가지 방법이 있으나, 최근 연구들은 대부분 아래와 같이 계산한다고 한다.  

$$idf(d, t) = \log \left( \frac{n}{1 + df(t)} \right)$$

계산 결과에 log를 취하는 이유는 문서의 크기 n이 커질수록 IDF값이 기하급수적으로 커지는 것을 방지하기 위해서이고, 분모에 1을 더하는 이유는 특정 단어가 전체 문서에서 등장하지 않을 경우에 분모가 0이 되는 상황을 방지하기 위한 것이라고 한다.  

## TF-IDF

---
## Reference
- [위키피디아: 단어 가방 모형](https://ko.wikipedia.org/wiki/%EB%8B%A8%EC%96%B4_%EA%B0%80%EB%B0%A9_%EB%AA%A8%ED%98%95)([영문](https://en.wikipedia.org/wiki/Bag-of-words_model))
- [위키피디아: TF-IDF](https://ko.wikipedia.org/wiki/Tf-idf)([영문](https://en.wikipedia.org/wiki/Tf%E2%80%93idf))
- [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/book/2155)
- [Understanding Inverse Document Frequency: On Theoretical Arguments for IDF](https://www.researchgate.net/publication/238123710_Understanding_Inverse_Document_Frequency_On_Theoretical_Arguments_for_IDF)